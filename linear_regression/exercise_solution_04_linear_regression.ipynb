{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Programming Task: Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a code skeleton for performing linear regression. \n",
    "Your task is to complete the functions where required. \n",
    "You are only allowed to use built-in Python functions, as well as any `numpy` functions. No other libraries / imports are allowed.\n",
    "\n",
    "In the beginning of every function there is docstring which specifies the input and and expected output.\n",
    "Write your code in a way that adheres to it.\n",
    "You may only use plain python and anything that we imported for you above such as numpy functions (i.e. no scikit-learn classifiers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we will work with the Boston Housing Dataset.\n",
    "The data consists of 506 samples. Each sample represents a district in the city of Boston and has 13 features, such as crime rate or taxation level. The regression target is the median house price in the given district (in $1000's).\n",
    "\n",
    "More details can be found here: http://lib.stat.cmu.edu/datasets/boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X , y = fetch_california_housing(return_X_y=True)\n",
    "\n",
    "# Add a vector of ones to the data matrix to absorb the bias term\n",
    "# (Recall slide #7 from the lecture)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "\"\"\"\n",
    "X 假设是一个二维数组（或称为矩阵），其中每一行代表一个观察值，每一列代表一个特征。\n",
    "\n",
    "X.shape[0] 获取X的行数，即观察值的数量。\n",
    "\n",
    "np.ones([X.shape[0], 1]) 创建一个形状为 (X.shape[0], 1) 的数组，这个数组的每一个元素都是1。这实际上创建了一个列向量，其中包含与X中的行数相同数量的1，通常这一列被称作截距项(intercept)或偏置项(bias)。\n",
    "\n",
    "np.hstack([...]) 将创建的全1列向量和原始的X数组水平堆叠起来。这意味着全1的列被添加到X的左边，作为第一列。\n",
    "\"\"\"\n",
    "# From now on, D refers to the number of features in the AUGMENTED dataset (i.e. including the dummy '1' feature for the absorbed bias term)\n",
    "\n",
    "# Split into train and test\n",
    "test_size = 0.9\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Fit standard linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_least_squares(X, y):\n",
    "    \"\"\"Fit ordinary least squares model to the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Regression targets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "        \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    \"\"\"\n",
    "    np.linalg.pinv(X) 计算矩阵 X 的伪逆（Moore-Penrose逆）。在普通最小二乘法中，当 X 的列数（特征数）小于行数（样本数）并且 X 的列向量（特征）线性无关时，可以使用标准的矩阵逆（np.linalg.inv(X.T @ X) @ X.T）来计算系数。但是，当 X 不是满秩的或者特征数大于样本数时，标准的矩阵逆不存在，这时候使用伪逆是更安全的选择，因为它总是存在。\n",
    "\n",
    "@ 是Python中的矩阵乘法运算符。它将 X 的伪逆矩阵与目标向量 y 相乘。\n",
    "np.linalg.pinv(X) @ y 的结果是系数向量 w，该向量最小化了残差平方和，即最小化了 \n",
    "在这里，w[0]（如果 X 被增广了的话）将是截距项，也就是偏置项。\n",
    "    \"\"\"\n",
    "    return np.linalg.pinv(X) @ y\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Fit ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_ridge(X, y, reg_strength):\n",
    "    \"\"\"Fit ridge regression model to the data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    y : array, shape [N]\n",
    "        Regression targets.\n",
    "    reg_strength : float\n",
    "        L2 regularization strength (denoted by lambda in the lecture)\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape [D]\n",
    "        Optimal regression coefficients (w[0] is the bias term).\n",
    "    \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return np.linalg.inv(X.T @ X + reg_strength * np.eye(X.shape[1])) @ X.T @ y\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Generate predictions for new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_linear_model(X, w):\n",
    "    \"\"\"Generate predictions for the given samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape [N, D]\n",
    "        (Augmented) feature matrix.\n",
    "    w : array, shape [D]\n",
    "        Regression coefficients.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : array, shape [N]\n",
    "        Predicted regression targets for the input data.\n",
    "        \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return X @ w\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Compute mean squared error between true and predicted regression targets.\n",
    "    \n",
    "    Reference: `https://en.wikipedia.org/wiki/Mean_squared_error`\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array\n",
    "        True regression targets.\n",
    "    y_pred : array\n",
    "        Predicted regression targets.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    mse : float\n",
    "        Mean squared error.\n",
    "        \n",
    "    \"\"\"\n",
    "    ### BEGIN SOLUTION ###\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "    ### END SOLUTION ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the two models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reference implementation produces \n",
    "* MSE for Least squares $\\approx$ **0.5347** , mean squared error 平方后要除以N\n",
    "\n",
    "* MSE for Ridge regression $\\approx$ **0.5331**\n",
    "\n",
    "You results might be slightly (i.e. $\\pm 1\\%$) different from the reference soultion due to numerical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Least squares = 0.5347102426013359\n",
      "MSE for Ridge regression = 0.5331285867787068\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "np.random.seed(1234)\n",
    "X , y = fetch_california_housing(return_X_y=True)\n",
    "X = np.hstack([np.ones([X.shape[0], 1]), X])\n",
    "test_size = 0.9 # we select a relatively large test set due to the large size of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n",
    "\n",
    "# Ordinary least squares regression\n",
    "w_ls = fit_least_squares(X_train, y_train)\n",
    "y_pred_ls = predict_linear_model(X_test, w_ls)\n",
    "mse_ls = mean_squared_error(y_test, y_pred_ls)\n",
    "print('MSE for Least squares = {0}'.format(mse_ls))\n",
    "\n",
    "# Ridge regression\n",
    "reg_strength = 1e-2\n",
    "w_ridge = fit_ridge(X_train, y_train, reg_strength)\n",
    "y_pred_ridge = predict_linear_model(X_test, w_ridge)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\"\"\"\n",
    "说明validation set中用了regularization 的误差更小\n",
    "\"\"\"\n",
    "print('MSE for Ridge regression = {0}'.format(mse_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
